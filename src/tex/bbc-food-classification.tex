\documentclass[11pt,a4paper]{article}

\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\usepackage{mathtools}
%\usepackage{harvard}
%\usepackage{jneurosci}

\title{Decision Tree Classification of Cuisines for BBC Food Recipes}
\author{Ross Fenning}

\begin{document}

\maketitle

\section{Introduction}

Most recipes in modern, British cooking have
originated in several different cultures and cuisines, such as Indian,
Italian or French. Some, like British and French, have arguably a lot
of crossover, whilst some are much more further apart.

Other cuisines might seem superficially different to people, such as Indian and
South-east Asian cuisines, but perhaps they overlap in a lot of their
ingredients due to relative geographical proximity?

\subsection{The Problem}

This coursework will explore applying supervised machine learning
techniques on food recipes and within it, we hope to create a model for
identifying the cultural origin of a recipe from the ingredients used
therein.

The BBC Food website provides a searchable collection of around 15,000
recipes created by professional TV chefs -- spanning twenty different
cuisines -- all of which comprise ingredients, preparation steps and
further metadata such as suitability for specific diets (e.g.
vegetarian) and expected time required to prepare the meal.

However, only around a third of the recipes have actually been manually
categorised as to which cuisine they are. Marking the remaining
two-thirds with their respective cuisine would be further manual effort
by editorial staff. A model that is able to assign or at least suggest
a cuisine based on supervised learning from the known set could be a useful
tool in improving the metadata on BBC Food recipes.

\subsection{Attributes}

A recipe can be said to have the following attributes:

\begin{itemize}
\item Ingredients
\item Preparation steps
\item Preparation time
\item Cooking time
\item Number of people it will serve
\item Cuisine
\item Special diets for which it may or may not be suitable
  (e.g. vegetarian, dairy-free)
\item Whether it's in season
\item For which course it is suitable (e.g. starter, dessert).
\item Whether it pertains to an occasion (e.g. Burn's Night, Christmas)
\item Which BBC programme has featured the recipe
\item Which chef created the recipe
\end{itemize}

For simplicity, we will focus initially on classifying by the cuisine attribute
(i.e. using that attribute as the class) based on the ingredients expressed as
vector of binary, nominal attributes, i.e. the ``carrot'' attribute is
\emph{true} if and only if the given recipe contains carrots.

Whether a recipe is in season is a transient attribute, but could possibly be
used in a different problem of learning which recipes are in season in different
times of the year. The chef and TV programmes that featured the recipe are
also out of scope for our problem, but could lead to an interesting machine
learning problem of trying to identify typical trends in recipes created by
particular chefs.

Occasion is categorical attribute that marks certain recipes as being typical
dishes served at particular festivals and celebrations. This could lead to
another classification problem, but for cuisine classification, it is likely to
be reasonably redundant due to the cultural basis of such festivals. For
instance, there are unlikely to be many Italian Burn's Night dishes nor Mexican
Diwali recipes.

The numerical attributes such as number of people the recipe is to serve or the
number of steps in the cooking method seem fairly arbitrary with respect to the
ingredients involved or the cuisine. This indicates the problem is very much
dealing with categorical attributes.

\section{BBC Food Dataset}

\subsection{Structure}

Figure~\ref{model} shows a simplified view of a data model for the BBC Food
domain. There are many more objects for functional aspects of the pages, but
the items included in the diagram capture enough to model the recipe content.

A recipe is broken up into \emph{stages} representing some discrete part of the
overall recipe, e.g. the filling and the pastry are two separate stages for
making a pie. Recipes that do not break up into stages default to having a single
stage that represents the whole dish.

It is the stages then that map to one or more \emph{ingredients} which represent
a specific quantity of a food item. For example, \emph{200g Flour} would be an
\emph{ingredient} with the flour being the \emph{food} and 200g being the
\emph{quantity}.

\begin{comment}
  @startuml food_data_model.png
  
  skinparam monochrome true
  skinparam circledCharacterRadius 0
  skinparam circledCharacterFontSize 0
  skinparam classFontSize 20
  
  class Recipe
  class Stage
  class Cuisine
  class Ingredient {
    quantity
  }
  class Food
  class Diet
  class Method
  class Chef {
    firstName
    surname
  }
  class Programme
  class Occasion
  
  Recipe "0..*" -u- "1" Cuisine
  Recipe "1" -d- "1..*" Stage
  Stage "1" -- "1..*" Ingredient
  Ingredient "1..*" -r- "1" Food
  Diet "0..*" -- "0..*" Recipe
  Recipe "1" -l- "1..*" Method
  Recipe "1..*" -- "1" Chef
  Recipe "1..*" -r- "0..*" Programme
  Recipe "0..*" -d- "0..*" Occasion
  Food "0..*" -u- "0..*" Occasion
  
  @enduml
\end{comment}
\begin{figure}[p]
  \begin{center}
    \includegraphics[width=\linewidth]{food_data_model.png}
  \end{center}
  \caption{Recipe domain model\label{model}}
\end{figure}

We can see that an \emph{Occasion} can be linked to either a food or a recipe.
For instance, \emph{Christmas} is linked both to the food \emph{mincemeat} and
the recipe \emph{Christmas turkey and stuffing}.

A recipe also comprises one or many \emph{methods}, which represent the list
of instructions to follow in order to prepare the dish. This is textual data
intended for human reading and is not useful as an attribute for machine
learning. It could be possible to massage some attributes out of these steps
using natural language techniques, but that is not explored here.

Finally, we can see that recipes also have the additional attributes of
diets, chefs and programmes.

Focusing on the cuisine attribute, Figure~\ref{cuisine-set} shows how many
recipes are assigned to a cuisine on the BBC Food website.

\begin{figure}[p]
  \includegraphics[width=\linewidth]{cuisine_set.png}
  \caption{Number of recipes that have a cuisine on BBC Food\label{cuisine-set}}
\end{figure}

It seems that approximately 30\% of the recipes on the BBC Food website are
assigned whilst 70\% haven't been categorised in this way. Given that one
such uncategorised recipe is
\emph{American herb pancakes with cottage cheese and pastrami}, which indicates
its country of origin in the title, it would appear that the cuisine categories
are missing due to a lack of time or resources to add them by hand. This
strengthens the utility of a model or system that could pre-populate the cuisine
category automatically.

\subsection{Visualisation of different classes}


\section{Classification}

As stated in the introduction, we will be exploring supervised learning techniques
on this recipe dataset and the primary focus will be developing an expert system
that can classify the cuisine from a recipe's ingredients. In this section,
we will be attempting to train such a classifier, evaluate it and then
look to ways to improve the model it produces.

Firstly, we can explore the dataset even further with respect to the cuisine class.
This starts with taking the relational structure shown in Figure~\ref{model} and
preparing the data to be ready for data mining and machine learning techniques.

\subsection{Flattening the recipes}

In machine learning, we are concerned with training models on sets of \emph{instances}
of a particular relation. It is therefore necessary to take largely structured
data such as that shown by Figure~\ref{model} and flatten or denormalise it into
a single relation. A single relation is then easy to express in CSV or ARFF file
formats for applications that can read them and provide analysis or mining:

\begin{equation} \label{recipe-tuple}
(cuisine, ingredient_1, ingredient_2, ... ,ingredient_N)
\end{equation}

In \eqref{recipe-tuple} all ingredients are converted to nominal, binary attributes
with values \emph{false} and \emph{true} to reflect whether that recipe contains
each of the possible ingredients or not. The cuisine attribute is a nominal
attribute which is serving as our class attribute.

Thus a single instance would be, for example:

\begin{equation} \label{recipe-example}
(Italian, True, False, ... ,True)
\end{equation}


\subsection{Exploring the Dataset}

The breakdown of the cuisine category is shown in Figure~\ref{cuisines-barchart}
for only those recipes that have a cuisine set.

\begin{figure}[p]
  \includegraphics[width=\linewidth]{cuisines.png}
  \caption{Number of recipes for each cuisine on BBC Food\label{cuisines-barchart}}
\end{figure}

British recipes are clearly dominant, which is likely not surprisingly for the
BBC, but it might also be a symptom of British cooking's tendancy to borrow and
derive from
many other cultures. A classification model might show deeper insight into which
cultures overlap most with British cuisine.

At the other end of the spectrum, there are some cuisines that have a very small
number of recipes. Portguese has the lowest at only 14 recipes and African
cooking has only 18 instances. These categories might be tricky when it comes to
classification as any model born out of a supervised learning process will not
have been exposed to many examples therein.

Looking at the ingredients attributes, we can see in Figure~\ref{top-ingredients}
that the number of recipes that contain the top 50 ingredients follows a
Pareto-looking curve when the ingredients themselves are arranged in descending
order of their appearance.

\begin{figure}[p]
  \includegraphics[width=\linewidth]{top_ingredients_counts.png}
  \caption{Number of recipes that use the top 50 ingredients\label{top-ingredients}}
\end{figure}

Perhaps unsurprisingly for a collection of professional recipes, olive oil is the most
common ingredient. The popular foods all seem to be very common ingredients to most
dishes, but this quickly descends to a long tail of foods that only appear in a few
of the recipes. In fact, 9\% of the foods only appear in a single recipe.
Figure~\ref{ingredients-counts} shows the full curve when we include all of the
ingredients.

It may be preferable clean the data of ingredients
with with too common an appearance as the amount of
self-information\cite{reza1961introduction} for a very common ingredient is low. For
instance if asked to guess the classification of an unseen recipe given only
the information that it contains olive oil, it is clear that little information
has been given. We can run any classification on both the full set of ingredients
and reduced versions thereof (i.e. projections into fewer dimensions) and compare
the models to evaluate the benefits of preprocessing the data in this way.

There may also be some benefit to reducing the dataset by removing the most rare
ingredients. Whilst their self-information is high (nearly 14 bits for ingredients
that only appear in one recipe compared to 0.8 bits for olive oil), they could
easily contribute to a model that overfits the data.

Overfitting occurs when the model becomes over-trained to the specific training set
used and becomes less useful for applying to new, unclassified instances. It has
been shown\cite{tetko1995neural} with neural network classifiers, that whilst the
classification error against a learning set gradually decreases over time, the
ability for that trained network to predict categories on instances outside of the
learning set follows a parabolic curve and starts to increase after passing a
minimum.

The likely explanation\cite{tetko1995neural} is that classifiers will find underlying
relations between attributes in the early iterations, but when the number of iterations
exceeds the optimum point, it is starting to learn noise and random correlations that
appear within that noise.

To use a real example from the recipe dataset, only one recipe in the set of
classified recipes uses tortellini as an ingredient: \emph{Tortellini in sarcofago}.
This recipe also happens to be classified as Italian cuisine, which means a classifier
that infers:

\begin{equation} \label{tortellini-rule}
has\_tortellini(X) \to italian(X)
\end{equation}

would be 100\% correct on that particular
rule given the training set, but it should be intuitively clear that the confidence
thereof is very low. This relates to the usage of \emph{pessimistic pruning} employed
by the C4.5 algorithm\cite{quinlan1993c4} on decision trees constructed by that algorithm.
Pessimistic pruning uses an upper bound on the probability of error for a given classification
based on the confidence limits for the binomial distribution.

If we follow the na\"ive approach used in pessimistic pruning and treat the error rate
of \eqref{tortellini-rule} as being $0$ error ``events'' out of $1$ ``trials'' (i.e.
we classified only one recipe with it and we were 100\% correct and 0\% wrong), then
the upper bound on likelihood of the rule being erroneous at 90\% confidence is
actually 95\% using the Clopper-Pearson method.\cite{clopper1934use}

Hopefully, it should be clear that it is probably suboptimal to build a a model from
a learned rule with a 0\% error rate on the training dataset, but could
have an error rate as high as 95\% based on an -- admittedly na\"ive -- application of
confidence intervals. Such a rule is a good indication of a model that has overfit
its learning dataset.

\begin{figure}[p]
  \includegraphics[width=\linewidth]{ingredients_counts.png}
  \caption{Descending frequency distribution of ingredients' appearances in recipes\label{ingredients-counts}}
\end{figure}

\subsection{Training a Cuisine Classifier}
\subsection{Evaluating the Model}
\subsection{Improving the Model}

\section{Discussion}
\subsection{Difficulties}
\subsection{Observations}
\subsection{Further Possibilties}

\bibliographystyle{cell}
\bibliography{bibtex}
\end{document}
