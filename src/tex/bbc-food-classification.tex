\documentclass[11pt,a4paper]{article}

\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{float}
\floatstyle{boxed}
%\restylefloat{figure}
\usepackage{mathtools}
\usepackage{rotating}
\usepackage{hyperref}
%\usepackage{rotfloat}
\title{Decision Tree Classification of Cuisines for BBC Food Recipes}
\author{Ross Fenning}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

Most recipes in modern, British cooking have
originated in several different cultures and cuisines, such as Indian,
Italian or French. Some, like British and French, have arguably a lot
of crossover, whilst some are much more further apart.

Other cuisines might seem superficially different to people, such as Indian and
South-east Asian cuisines, but perhaps they overlap in a lot of their
ingredients due to relative geographical proximity?

\subsection{The Problem}

This coursework will explore applying supervised machine learning
techniques on food recipes and within it, we hope to create a model for
identifying the cultural origin of a recipe from the ingredients used
therein.

The BBC Food website provides a searchable collection of around 15,000
recipes created by professional TV chefs -- spanning twenty different
cuisines -- all of which comprise ingredients, preparation steps and
further metadata such as suitability for specific diets (e.g.
vegetarian) and expected time required to prepare the meal.

However, only around a third of the recipes have actually been manually
categorised as to which cuisine they are. Marking the remaining
two-thirds with their respective cuisine would be further manual effort
by editorial staff. A model that is able to assign or at least suggest
a cuisine based on supervised learning from the known set could be a useful
tool in improving the metadata on BBC Food recipes.

Further to the scope of this problem, an expert system with learnt knowledge
about food, recipes and cuisine might be able to suggest new recipes
not yet created by a human. Another use might be to suggest suitable things to make
based on what ingredients a person has or what is currently in season. Perhaps
another system could suggest ways to tweak a recipe to make it suitable for
vegetarians or plan meals for an occasion based on dietary or monetary contraints.

\subsection{Attributes}

A recipe can be said to have the following attributes:

\begin{itemize}
\item Ingredients
\item Preparation steps
\item Preparation time
\item Cooking time
\item Number of people it will serve
\item Cuisine
\item Special diets for which it may or may not be suitable
  (e.g. vegetarian, dairy-free)
\item Whether it's in season
\item For which course it is suitable (e.g. starter, dessert).
\item Whether it pertains to an occasion (e.g. Burn's Night, Christmas)
\item Which BBC programme has featured the recipe
\item Which chef created the recipe
\end{itemize}

For simplicity, we will focus initially on classifying by the cuisine attribute
(i.e. using that attribute as the class) based on the ingredients expressed as
vector of binary, nominal attributes, i.e. the ``carrot'' attribute is
\emph{1} if and only if the given recipe contains carrots (and \emph{0} otherwise).

Whether a recipe is in season is a transient attribute, but could possibly be
used in a different problem of learning which recipes are in season in different
times of the year. The chef and TV programmes that featured the recipe are
also out of scope for our problem, but could lead to an interesting machine
learning problem of trying to identify typical trends in recipes created by
particular chefs.

Occasion is a categorical attribute that marks certain recipes as being typical
dishes served at particular festivals and celebrations. This could lead to
another classification problem, but for cuisine classification, it is likely to
be reasonably redundant due to the cultural basis of such festivals. For
instance, there are unlikely to be many Italian Burn's Night dishes nor Mexican
Diwali recipes.

The numerical attributes such as number of people the recipe is to serve or the
number of steps in the cooking method seem fairly arbitrary with respect to the
ingredients involved or the cuisine. This makes all the numeric attributes less
desirable for the categorisation problem in hand.

\begin{comment}
  @startuml food_data_model.png
  
  skinparam monochrome true
  skinparam circledCharacterRadius 0
  skinparam circledCharacterFontSize 0
  skinparam classFontSize 20
  
  class Recipe
  class Stage
  class Cuisine
  class Ingredient {
    quantity
  }
  class Food
  class Diet
  class Method
  class Chef {
    firstName
    surname
  }
  class Programme
  class Occasion
  
  Recipe "0..*" -u- "1" Cuisine
  Recipe "1" -d- "1..*" Stage
  Stage "1" -- "1..*" Ingredient
  Ingredient "1..*" -r- "1" Food
  Diet "0..*" -- "0..*" Recipe
  Recipe "1" -l- "1..*" Method
  Recipe "1..*" -- "1" Chef
  Recipe "1..*" -r- "0..*" Programme
  Recipe "0..*" -d- "0..*" Occasion
  Food "0..*" -u- "0..*" Occasion
  
  @enduml
\end{comment}
\begin{figure}[p]
  \begin{center}
    \includegraphics[width=\linewidth]{food_data_model.png}
  \end{center}
  \caption{Recipe domain model\label{model}}
\end{figure}

\section{BBC Food Dataset}

\subsection{Structure}

Figure~\ref{model} shows a simplified view of the data model for the BBC Food
domain. There are many more objects for functional aspects of the pages, but
the items included in the diagram capture enough to model the recipe content itself.

A recipe is broken up into \emph{stages} representing some discrete part of the
overall recipe, e.g. the filling and the pastry are two separate stages for
making a pie. Recipes that do not break up into stages default to having a single
stage that represents the whole dish.

It is the stages then that map to one or more \emph{ingredients} which represent
a specific quantity of a food item. For example, \emph{200g Flour} would be an
\emph{ingredient} with the flour being the \emph{food} and 200g being the
\emph{quantity}. Throughout this report, the term ingredient will be used to
represent both a food item and its use in a dish as there is little need to
make the strict distinction as in the data model after we have flattened to a
single relation for the machine learning process.

We can see that an \emph{Occasion} can be linked to either a food or a recipe.
For instance, \emph{Christmas} is linked to both the food \emph{mincemeat} and
the recipe \emph{Christmas turkey and stuffing}.

A recipe also comprises one or many \emph{methods}, which represent the list
of instructions to follow in order to prepare the dish. This is textual data
intended for human reading and is not useful as an attribute for machine
learning. It could be possible to massage some attributes out of these steps
using natural language techniques, but that is not explored here.

Finally, we can see that recipes also have the additional attributes of
diets, chefs and programmes.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{cuisine_set.png}
  \caption{Number of recipes that have a cuisine on BBC Food\label{cuisine-set}}
\end{figure}

Focusing on the cuisine attribute, Figure~\ref{cuisine-set} shows how many
recipes are assigned to a cuisine on the BBC Food website.

It seems that approximately 30\% of the recipes on the BBC Food website are
assigned whilst 70\% haven't been categorised in this way. Given that one
such uncategorised recipe is
\emph{American herb pancakes with cottage cheese and pastrami}, which indicates
its country of origin in the title, it would appear that the cuisine categories
are missing due to a lack of time or resources to add them by hand. This
strengthens the utility of a model or system that could pre-populate the cuisine
category automatically.

\subsection{Visualisation of different classes}
\label{visualisation}
Visualisation of relations with of the order of 1,000 attributes is not as
trivial a plotting 2-dimensional vectors in a plane and a lot of visualisation
techniques have historically been for numeric data. A popular technique that
has risen with the increase of textual and higher-dimensional data from places
such as social networks is the \emph{tag cloud}. \cite{han2006data}

\begin{figure}[b!]
  \includegraphics[width=\linewidth]{ingredients_tags.png}
  \caption{Tag cloud showing popularity of ingredients across all recipes}
  \label{fig:tagcloud}
\end{figure}

\begin{figure}[p]
  \includegraphics[width=\linewidth]{indian_ingredients_tags.png}
  \caption{Tag cloud showing popularity of ingredients for Indian recipes}
  \label{fig:tagcloud-indian}
\end{figure}

\begin{figure}[p]
  \includegraphics[width=\linewidth]{italian_ingredients_tags.png}
  \caption{Tag cloud showing popularity of ingredients for Italian recipes}
  \label{fig:tagcloud-italian}
\end{figure}

Such a tag cloud can be seen in figure~\ref{fig:tagcloud}. There is a clear
abundance of black pepper and olive oil and we can see, at a glance, an
impression of all the ingredients that appear in the most recipes. Where
this visualisation becomes even more useful is when compared side-by-side
with the same tag cloud narrowed down to only, say, Indian cusines, as
shown in figure~\ref{fig:tagcloud-indian}.

The Italian-only tag cloud in figure~\ref{fig:tagcloud-italian} has
a very different aesthetic in that it generally looks a lot more sparse. This
seems to be due to having a few, very prominent ingredients common
to nearly all recipes, but perhaps a longer tail of the less common
ingredients. This may indicate Italian as being a more diverse cuisine or
that Indian has a larger number of ingredients that are prevalent in all
of its typical dishes. The tag cloud approach to visualisation gives us
a very quick way to see the ingredient distributions across different
subsets of the recipe dataset and is a good way to surface such possible
explanations for the differences we can see.

The tag cloud also gives a quick way to view subsets of recipes by other
criteria, e.g. recipes marked as pertaining
to Christmas as in figure~\ref{fig:tagcloud-christmas}.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{christmas_ingredients_tags.png}
  \caption{Tag cloud showing popularity of ingredients for Christmas}
  \label{fig:tagcloud-christmas}
\end{figure}

\section{Classification}

As stated in the introduction, we will be exploring supervised learning techniques
on this recipe dataset and the primary focus will be developing an expert system
that can classify the cuisine from a recipe's ingredients. In this section,
we will be attempting to train such a classifier, evaluate it and then
look to ways to improve the model it produces.

Firstly, we can explore the dataset even further with respect to the cuisine class.
This starts with taking the relational structure shown in Figure~\ref{model} and
preparing the data to be ready for data mining and machine learning techniques.

\subsection{Flattening the recipes}

In machine learning, we are concerned with training models on sets of \emph{instances}
of a particular relation. It is therefore necessary to take largely structured
data such as that shown by Figure~\ref{model} and flatten or denormalise it into
a single relation. A single relation is then easy to express in CSV or ARFF file
formats for applications that can read them and provide analysis or mining. For this
problem, our relation will be:

\begin{equation} \label{recipe-tuple}
(cuisine, ingredient_1, ingredient_2, ... ,ingredient_N)
\end{equation}

In \eqref{recipe-tuple} all ingredients are converted to nominal, binary attributes
with values \emph{0} and \emph{1} to reflect whether that recipe contains
each of the possible ingredients or not. The cuisine attribute is a nominal
attribute which is serving as our class attribute.

Thus a single instance would be, for example:

\begin{equation} \label{recipe-example}
(Italian,1,0,...,1)
\end{equation}

This easily translates into CSV format for plotting and other applications, but if
we wish to use the Weka machine learning application, we would be better producing
our relation in ARFF format. \cite{witten2011data} An example of an ARFF file with only two
instances of the relation is shown in figure~\ref{fig:arff}.

\begin{figure}[h]
  \verbatiminput{example.arff}
  \caption{Example ARFF file showing recipe relation with two instances}
  \label{fig:arff}
\end{figure}

The Caribbean line has more instances of \emph{0} than \emph{1} and this is in
fact very much the case for all recipes across the whole set. It is shown in
section~\ref{sec:distances} that the dataset is very sparse and thus a
sparse ARFF file is more practical. \cite{witten2011data} An example is shown
in figure~\ref{fig:arff-sparse}

\begin{figure}[h]
  \verbatiminput{example_sparse.arff}
  \caption{Example sparse ARFF file showing recipe relation with two instances}
  \label{fig:arff-sparse}
\end{figure}

With the relation flattened and prepared to import into different applications
or bespoke code, we can begin to explore the dataset in more detail.

\subsection{Exploring the Dataset}
\label{sec:exploring}

The breakdown of the cuisine category is shown in Figure~\ref{cuisines-barchart}
for only those recipes that have a cuisine set.

\begin{figure}[p]
  \includegraphics[width=\linewidth]{cuisines.png}
  \caption{Number of recipes for each cuisine on BBC Food\label{cuisines-barchart}}
\end{figure}

British recipes are clearly dominant, which is likely not surprisingly for the
BBC, but it might also be a symptom of British cooking's tendancy to borrow and
derive from
many other cultures. A classification model might show deeper insight into which
cultures overlap most with British cuisine.

At the other end of the spectrum, there are some cuisines that have a very small
number of recipes. Portuguese has the lowest at only 14 recipes and African
cooking has only 18 instances. These categories might be tricky when it comes to
classification as any model born out of a supervised learning process will not
have been exposed to many examples therein.

Looking at the ingredients attributes, we can see in Figure~\ref{top-ingredients}
that the number of recipes that contain the top 50 ingredients follows a
Pareto-looking curve when the ingredients themselves are arranged in descending
order of their appearance.

\begin{figure}[p]
  \includegraphics[width=\linewidth]{top_ingredients_counts.png}
  \caption{Number of recipes that use the top 50 ingredients\label{top-ingredients}}
\end{figure}

Perhaps unsurprisingly for a collection of professional recipes, olive oil is the most
common ingredient. The popular foods all seem to be very common ingredients to most
dishes, but this quickly descends to a long tail of foods that only appear in a few
of the recipes. In fact, 9\% of the foods only appear in a single recipe.
Figure~\ref{ingredients-counts} shows the full curve when we include all of the
ingredients.

\begin{figure}[p]
  \includegraphics[width=\linewidth]{ingredients_counts.png}
  \caption{Descending frequency distribution of ingredients' appearances in recipes\label{ingredients-counts}}
\end{figure}

\subsection{Data Reduction}
\label{sec:data-reduction}
It may be preferable to clean the data of ingredients
with too common an appearance as the amount of
self-information \cite{reza1961introduction} for a very common ingredient is low. For
instance if asked to guess the classification of an unseen recipe given only
the information that it contains olive oil, it is clear that little information
has been given to aid classification. We can run any classification on
both the full set of ingredients
and reduced versions thereof (i.e. projections into fewer dimensions) and compare
the models to evaluate the benefits of preprocessing the data in this way.

There may also be some benefit to reducing the dataset by removing the most rare
ingredients. Whilst their self-information is high (nearly 14 bits for ingredients
that only appear in one recipe compared to 0.8 bits for olive oil), they could
easily contribute to a model that overfits the data.

Overfitting occurs when the model becomes over-trained to the specific training set
used and becomes less useful for applying to new, unclassified instances. It has
been shown \cite{tetko1995neural} with neural network classifiers, that whilst the
classification error against a learning set gradually decreases over time, the
error for that trained network's ability to predict categories on instances outside of the
learning set can follow a parabolic curve that starts to increase after passing an
optimal minimum.

The likely explanation \cite{tetko1995neural} is that classifiers will find underlying
relations between attributes in the early iterations, but when the number of iterations
exceeds the optimum point, it is starting to learn noise and random correlations that
appear within that noise.

To use a real example from the recipe dataset, only one recipe in the set of
classified recipes uses tortellini as an ingredient: \emph{Tortellini in sarcofago}.
This recipe also happens to be classified as Italian cuisine, which means a classifier
that infers:

\begin{equation} \label{tortellini-rule}
has\_tortellini(X) \to italian(X)
\end{equation}

\noindent would be 100\% correct on that particular
rule given the training set, but it should be intuitively clear that the confidence
thereof is very low. This relates to the usage of \emph{pessimistic pruning} employed
by the C4.5 algorithm \cite{quinlan1993c4} on decision trees constructed by that algorithm.
Pessimistic pruning uses an upper bound on the probability of error for a given classification
based on the confidence limits for the binomial distribution.

If we follow the na\"ive approach used in pessimistic pruning and treat the error rate
of \eqref{tortellini-rule} as being $0$ error ``events'' out of $1$ ``trials'' (i.e.
we classified only one recipe with it and we were 100\% correct and 0\% wrong), then
the upper bound on likelihood of the rule being erroneous at 90\% confidence is
actually 95\% using the Clopper-Pearson method. \cite{clopper1934use}

Hopefully, it should be clear that it is probably suboptimal to build a model from
a learned rule that could
have an error rate up to 95\% based on an -- admittedly na\"ive -- application of
confidence intervals and a pessimistic interpretation thereof.
Such a rule is a good indication of a model that has overfit
its learning dataset and is justification for either post-pruning using Quinlan's
pessimistic pruning or preventing through a data reduction step that removes rare
ingredients before any data mining even takes place.

In tandem with the desire to prevent overfitting of the data, is a desire to build
the smallest model or tree possible that best classifies the recipes. This
aim is expressed both by Quinlan when introducing C4.5 \cite{quinlan1993c4}
and by Witten and Frank \cite{witten2011data}, who introduce the
\emph{Minimum Description Length}, or MDL, principle. This principle is
described as being a manifestation of \emph{Occam's Razor} within machine learning
in that we prefer the simplest theory (i.e. classification model) that describes
the evidence (i.e. the dataset). They describe the goal of trying to choose
theory $T$ given the evidence $E$ that maximises the posteriori probability:

\begin{equation}
P(T | E)
\end{equation}

\noindent as being equivalent to trying to minimise the number of bits -- i.e. the length $L$
 -- necessary to describe the theory and its informational loss:

\begin{equation}
L(T) + L(E | T)
\end{equation}

It could be that one way to ensure we achieve the smallest model possible is eagerly to remove
attributes and dimensions that we can predict with some confidence are unlikely
to contribute to the final expert system produced.

It may be the case however that little is gained by preemptively removing
ingredients that are unlikely to be helpful for classification. If we are to use an Inductive
Decision Tree classification approach such as ID3 \cite{quinlan1986induction}
or C4.5 \cite{quinlan1993c4}, then the algorithm should endeavour to build
rules based on ingredients that best classify the cuisines. It should be expected
in an optimal solution therefore that preemptive reduction of the ingredients
attributes is unnecessary as the less indicative ingredients will not be used
near the top of the tree. We would then achieve a minimal model but halting
the tree construction at the error minimum point before overfitting starts
to occur as described by Tetko, Livingstone and Luik, \cite{tetko1995neural}
which should be the point at which the attributes that have genuine relationships
have been modelled, but before the point at which outlier attributes such as
\emph{tortellini} above (which could be considered noise) start to affect the
classification.

Unfortunately, the assumption that the algorithm alone will behave optimally in this
way cannot be relied upon. Constructing an optimal decision tree is shown to be
an NP-complete problem \cite{hyafil1976constructing}, which is why algorithms
like C4.5 are greedy and nonbacktracking \cite{quinlan1993c4}, favouring
maximising local optima only at each iteration. An ancestor to C4.5, CLS,
uses a lookahead to a fixed depth similar to Minimax \cite{quinlan1986induction},
but this can be computational expensive. Given we usually want supervised
learning to scale to large numbers of learning examples, it is arguably preferable
to take the more common approach of aiding the learning process with
data reduction -- e.g. removing ingredients that don't match some acceptance threshold
as suggested above -- or divide-and-conquer approaches where perhaps clustering is
used to group recipes that are similar so that different classifiers can be
trained on those specific clusters only. It is also common in machine learning
to use expert domain knowledge where possible to guide improvements to the
models.

For the cuisine classification problem, we will define \emph{minimum occurrence}
and \emph{maximum occurrence} thresholds below and above which we will remove
ingredients attributes. They will be set initially to $O$ and $\infty$
respectively so the first models will be constructed from every ingredient
attribute, but they will then provide a baseline against which we can compare
models learnt from different occurrence thresholds so as to evaluate whether
improvements are made through data reduction in this way.

Another consideration for data reduction can be to reduce dimensionality by creating
compound attributes from ingredients that strongly correlate. For instance, if
we were to decide either through similarity calculations or domain knowledge
that salt and pepper always appear in recipes together and negligibly does
one appear without the other, we can replace these distrinct attributes with a compound
attribute \emph{salt-pepper} that is \emph{1} for all recipes that has either.
Again, this reduces complexity and can even smooth out noise \cite{han2006data} and
thus do more to increase accuracy than to reduce it.

\subsection{Similarity and Dissimilarity}
\label{sec:distances}
Given all the ingredients attributes are asymmetric, binary attributes, we can ignore
the cuisine class attribute for now and calculate the similarity or distances
between recipes using the \emph{Jaccard distance}: \cite{han2006data}

\begin{equation}
J_{\delta}(A,B) = 1 - J(A,B) = { { |A \cup B| - |A \cap B| } \over |A \cup B| }
\end{equation}

\noindent where $A$ and $B$ are the sets of ingredients that appear in two given
recipes. We can see such a matrix on a sample of five reasonably similar recipes
in Table~\ref{tab:distance-matrix}.

% ('amarettovanillacusta_70907', 'cremebrulee_91228', 'raspberryrippleicecr_91970', 'apricotupsidedowntar_91748', 'americanpancakes_72713')

\begin{sidewaystable}
  \centering
  \begin{tabular}{| p{6cm} | p{2.5cm} | p{2.5cm} | p{2.5cm} | p{2.5cm} | p{2.5cm} |}
    \hline
                               & \textbf{Amaretto vanilla custard} & \textbf{Cr\`eme br\^ul\'ee} & \textbf{Raspberry ripple ice cream} & \textbf{Apricot upside-down tart} & \textbf{American pancakes} \\
    \hline
    \textbf{Amaretto vanilla custard}   & 0                        & 0                  & 0.1667                     & 0.9                      & 0.9167            \\
    \hline
    \textbf{Cr\`eme br\^ul\'ee}         & 0                        & 0                  & 0.1667                     & 0.9                      & 0.9167            \\
    \hline
    \textbf{Raspberry ripple ice cream} & 0.1667                   & 0.1667             & 0                          & 0.9091                   & 0.9231            \\
    \hline
    \textbf{Apricot upside-down tart}   & 0.9                      & 0.9                & 0.9091                     & 0                        & 0.7273            \\
    \hline
    \textbf{American pancakes}          & 0.9167                   & 0.9167             & 0.9231                     & 0.7273                   & 0                 \\
    \hline
  \end{tabular}
  \caption{Distance matrix between five dessert recipes}
  \label{tab:distance-matrix}
\end{sidewaystable}

One immediate observation from Table~\ref{tab:distance-matrix} is that there
are two recipes that come out with a distance of $0$, implying they have
identical ingredients. This appears to be indeed the case. Even more interestingly,
the cr\`eme br\^ul\'ee is classified as \emph{French} whereas the
amaretto vanilla custard is classified as \emph{British}. It is clear that any classification
of the cuisine based solely on the ingredients is going to put these two recipes in
the same class (although which it will choose is less clear) and thus we can see
\emph{before even running any supervised learning} that there will be errors when using
just ingredients as the training attributes. It is also particularly of note when
we refer to domain knowledge on
cooking and cuisine that our first such case is a confusion between a French and a British
dish given these are two cuisines with significant overlap from centuries of cultural mixing
and proximity.

A further observation is that the sparse nature of the number of ingredients used
on average per recipe out of all possible ingredients means that it is very easy for two
recipes that have a few ingredients in common to come out with a large distance apart such
as 0.9. The mean distance between any two classified recipes in this dataset is 0.9381, which
indicates a very sparse dataset in general.

% black_pepper
% olive_oil
% strong_white_flour
% yeast
% coriander_seeds
% cumin
% guacamole
% salsa
\begin{sidewaystable}
  \centering
  \begin{tabular}{| p{3.8cm} | p{1.5cm} | p{1.5cm} | p{1.5cm} | p{1.3cm} | p{1.9cm} | p{1.3cm} | p{2.1cm} | p{1.3cm} |}
    \hline
    & \textbf{Black Pepper} & \textbf{Olive Oil} & \textbf{Strong White Flour} & \textbf{Yeast}  & \textbf{Coriander Seeds} & \textbf{Cumin}  & \textbf{Guacamole} & \textbf{Salsa}  \\
    \hline
    \textbf{Black Pepper}       & 0            & 0.4684    & 0.9931             & 0.9905 & 0.9600          & 0.9410 & 0.9997    & 0.9997 \\
    \hline
    \textbf{Olive Oil}          & 0.4684       & 0         & 0.9907             & 0.9876 & 0.9621          & 0.9419 & 0.9997    & 0.9997 \\
    \hline
    \textbf{Strong White Flour} & 0.9931       & 0.9907    & 0                  & 0.5733 & 0.9896          & 0.9933 & 1         & 1      \\
    \hline
    \textbf{Yeast}              & 0.9905       & 0.9876    & 0.5733             & 0      & 0.9864          & 0.9916 & 1         & 1      \\
    \hline
    \textbf{Coriander Seeds}    & 0.9600       & 0.9621    & 0.9896             & 0.9864 & 0               & 0.6630 & 1         & 1      \\
    \hline
    \textbf{Cumin}              & 0.9410       & 0.9419    & 0.9933             & 0.9916 & 0.6630          & 0      & 1         & 1      \\
    \hline
    \textbf{Guacamole}          & 0.9997       & 0.9997    & 1                  & 1      & 1               & 1      & 0         & 0.6667 \\
    \hline
    \textbf{Salsa}              & 0.9997       & 0.9997    & 1                  & 1      & 1               & 1      & 0.6667    & 0      \\
    \hline
  \end{tabular}
  \caption{Distance matrix between eight ingredients}
  \label{tab:food-distance-matrix}
\end{sidewaystable}

Table~\ref{tab:food-distance-matrix} shows an example distance matrix between eight
different food ingredients. The closest two in the whole dataset are olive oil
and black pepper, which were in section~\ref{visualisation} to be
incredibly common across all recipes.
We can see some pairings that would be expected from cooking knowledge, such as
cumin and coriander (both common in Indian cuisine) as well as salsa and guacamole
(both frequently present in Mexican food). Note that there is a strong disconnect
between the Indian spices and the Mexican condiments, which suggests no recipe
in our training set has ever combined cumin and salsa, for example.

Strong, white flour comes out very close to yeast, which is an influence from the large
number of cakes and breads recipes in the BBC Food repertoire. The mean distance
between every ingredient is approximately 0.997, which makes ingredients seem
even more sparse than the recipes.

\subsection{Induction Decision Tree Classification}
\label{sec:idt-classification}

As we will be trying an Inductive Decision Tree classification, we can refer
initially to Quinlan's ID3 algorithm. \cite{quinlan1986induction} This is a top-down,
recursive approach where we choose an attribute (i.e. an ingredient in this case),
split the dataset into sets that each share values of that attribute (i.e. a
set of recipes that have and a set that do not have the ingredient) and
then repeat the process on those subsets. The recursion halts when there are no
further ways to split the set or if everything in the set has the same class.

The choice of which attribute to use to determine the split is a crucial part of
any top-down, inductive tree classifier. In the case of ID3, an information-based
approach is taken. This starts by calculating the entropy \cite{reza1961introduction}
of the dataset:

\begin{equation}
  I(D)= - \sum_{i=1}^mp_i\log_2(p_i)
\end{equation}

\noindent where $D$ is the dataset of all recipes with $m$ cuisines
and $p_i$ is the nonzero probability
that an arbitrary recipe in $D$ belongs to cuisine $i$. Initially, the set of
classified recipes has an entropy of $3.0195 bits$. This represents the information
that would be required on average to classify a recipe. We then try to find
an ingredient on which we can split the dataset and calculate the entropies thereof.
The amount of information still required to classify an exact cuisine after
partitioning $P$ can be expressed as a weighted average of the respective
entropies:

\begin{equation}
  I_P(D) = \sum_{j=1}^n\frac{|D_j|}{D} \times I(D_j)
\end{equation}

\noindent where $D$ has been split into $n$ partitions and $D_j$ is each of such
partitions. Note that given ingredients are binary attributes, it will
always be the case that $n = 2$ as we can only ever split recipe sets into
those that do and those that do not have a given ingredient.

The amount of information we have gained is this equal to the reduction in
information required to classify after the partitioning:

\begin{equation}
  Gain(P) = I(D) - I_P(D)
\end{equation}

If we choose, for example, to split the recipe on whether it contains the
ingredient \emph{gravy}, the entropy of the set without gravy is $3.0246$
and the set of recipes with gravy has entropy $0.9568$. The information
gain here is $0.0044$, but we can likely do better than choosing an
ingredient at random. If we iterate through every possible ingredient, we
find the best split is in fact on \emph{ginger}, with an information gain
of $0.2129$.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=\linewidth]{tree.png}
  \end{center}
  \caption{First few layers of decision tree from ID3 classification\label{stub-decision-tree}}
\end{figure}

Running this with an arbitrary depth limit gives the tree shown in
Figure~\ref{stub-decision-tree}. The leaves in green represent where
the ID3 algorithm would naturally place a classification leaf for that
cuisine whereas the leaves in red show where a subtree would be built
were it not for the artifical depth limit put in place. The cuisine
class placed in the latter represents the majority.

Also at each classification leaf, we can
see correct/incorrect classification counts similar to that produced
by the C4.5 algorithm. The first number shows the number of recipes
correctly classified with that the given cuisine and the second number
indicates the number of recipes that are of a different cuisine that
erroneously get classified by that leaf. These numbers not only give
an idea of the error rate of the generated model, but also feed into the
\emph{pessimistic pruning} \cite{quinlan1993c4} that C4.5 employs
to decide whether the error rate's maximum within an confidence interval is
high enough to justify pruning whole subtrees. A subtree pruned in this
way is replaced by the majority class therein.

In our tree so far, we can see that on the ``yes'' branch of ``fish sauce?'',
there are a total of 18 Thai/South-East Asian dishes and only 3 dishes
in other cuisines. The classifications for other cuisines are based on
very small numbers that raise concerns about statistical confidence and
overfitting raised in the section~\ref{sec:data-reduction}. It is subtrees such as
this one where C4.5's pessimistic pruning may consider replacing
with a single leaf classifying everything thereunder as Thai/South-East Asian.
Again, as discussed in the section~\ref{sec:data-reduction}, simplifying our decision tree
model in this way may do more to smooth out noise and increase confidence in the
error rates of the leaves than to harm the accuracy. Pruning our
model in this way could well provide a better classifier for unseen recipes outside
of the training set.

\subsection{Classifiying with Weka}
\label{sec:weka}

Rather than reinvent the wheel and step through different combinations
of algorithms, parameters and prunings, we can make use of the Weka \cite{witten2011data}
machine learning suite developed at the University of Waikato, New Zealand.
We can provide our recipes relation in ARFF format and use pre-built Java
implementations of various classification algorithms.

Firstly, we can run a vanilla ID3 tree classifier, which is essentially the full
conclusion of the tree we started building in section~\ref{sec:idt-classification}.
There are no real parameters to tweak this algorithm and we will simply test it on the
same set on which it was trained for now.

The tree generated is very deep and the resulting model correctly classifies 99.9369\%
of recipes (all but three). However, it is very likely this has overfit the exact
recipes provided and it might not be useful for classifying new recipes. It is
however, a good first run for validating what was expected to happen. As in
section~\ref{sec:idt-classification} we can see that the Weka implementation of ID3
has also chosen ginger as the first split and that the top few levels
of the tree match figure~\ref{stub-decision-tree}.

Given the assumption that was made beforehand that running ID3
(without either reducing the data or pruning the tree) would overfit the
data and model too much of the noise, what can we say about the three recipes that it
still couldn't classify? The full confusion matrix is not too interesting, but
the incorrectly classified recipes were all ones erroneously marked as British
dishes, whereas one was actually Italian and two French. Nothing surprising here
that the first errors are confusing between similar European cuisines. French and
Italian cuisines also both heavily influence modern, British cooking.

Before looking at other algorithms, pruning or reducing the noise from the dataset,
we shall explore how to evaluate the ID3-generated tree a little more accurately.
When the accuracy comes out near 100\%, it looks as if the model is already working!
Since we know that's unlikely to be the case, let's try and demonstrate that.

\subsection{Splitting the training set}

Instead of the somewhat circular approach of training and then evaluating a model on the
same set, one approach is to have separate test data to the training data.

Having separate training and test data is a good way to ensure your model has not
overfit the training examples. A tree that has overfit all the noise and randomness of a training
set will perform less well on any test examples. \cite{tetko1995neural} Weka provides
an easy way to split the dataset into training and test data, so we can do a
second run of ID3 with a 50\% split on the dataset in this way.

The correct classification rate is now down to 48.3396\%, so it's struggling a bit to infer
the cuisines of one half of the set using the other half. One downside of splitting the
training set is that our model was retrained on fewer examples than in section~\ref{sec:weka}.
Perhaps halving the number
of training examples was not ideal for increasing the accuracy. We can try again with only
25\% of the recipes put aside for testing. This brings the accuracy up to 50.3785\%, which
is better, but perhaps not significantly so.

Finally, we can try putting aside only 10\% of the dataset to be used as test data
and the accuracy climbs a little to 53.1513\%. These are small improvements, but a trend
does seem to be present that removing items from the training examples is harming the
accuracy as the supervised learning process has fewer datapoints with which to build a
model. More importantly, however, is the fact that the accuracy seems to be no
higher than around the 54\% no matter how the set is split. This suggests a significant
error rate likely due to overfitting.

\subsection{Cross-validation}

We can get a better idea of the general accuracy/error rate for the ID3-based model
by applying \emph{K-folds} cross-validation. In short we can achieve an estimate of
the prediction accuracy of our classifier by repeating the training and testing
phases over $K$ iterations, where in each iteration a $\frac{1}{K}$ subset of the dataset
is taken out for validation. Essentially, we ultimately use every training example
both for training and for testing.

Weka allows us to rerun our ID3 classification using cross-validation in the testing
phase. A 10-fold cross validation is chosen for no more reason than it is a reasonably
popular value for $K$. \cite{mclachlan2005analyzing} The accuracy comes out at
51.7234\% of recipes classified correctly, which supports further the idea that
the ID3-based model is not going to perform much better than that on real, unseen
recipes and may have overfit the training data.

Before switching to algorithms that make use of tree pruning, we can first explore the
suggestion of removing attributes from the dataset as suggested in
section~\ref{sec:data-reduction}.

\subsection{Applying data reduction}

In section~\ref{sec:data-reduction}, it was discussed as to whether noise could be
avoided by removing attributes that appear too infrequently (or too frequently) to be
useful classifiers. The hypothesis was that we could use \emph{minimum occurrence} and
\emph{maximum occurrence} thresholds to determine whether an ingredient attribute
is used in our training set.

It is simple enough to prepare the data in ARFF format again, but drop attributes from
the relation that represent ingredients that appear in too few or too many recipes.
Setting the minimum occurrence threshold to 10 (and continuing to use cross-validation)
produced a model with a predicted accuracy of 51.3241\% and setting the maximum
occurrence threshold to 1,000 along with that resulted in an accuracy of 52.5641\%. It
seems to be that case that any dimensional-reduction in the dataset is not having a
large benefit to the model generated by ID3.

As also discussed in section~\ref{sec:data-reduction}, it is not unreasonable
to trust the information gain heuristic in ID3 and related algorithms to choose
to split on attributes that are informative in classifying recipes and avoid
-- at least initially -- the attributes that are of little use. A better
approach to improving the model's accuracy on unseen recipes might be
to look at pruning the tree of branches that actually contribute more to the error
(e.g. if they are symptoms of overfitting and are inferring relationships that
aren't actually there in the domain).

\subsection{Pruning the Tree}
\label{sec:pruning}

Other inductive tree algorithms related to ID3 such as C4.5 and CART employ some
level of tree \emph{pruning}. In this context, pruning means to identify a node
representing a subtree that has been or will be built and replace it instead with
a leaf. Preventing a subtree
from being built (i.e. by halting the recursion for some reason other than 
the normal base cases) is known as \emph{prepruning} and removing subtrees from
a fully-built tree is known as \emph{postpruning}.

The C4.5 algorithm and its Java implementating in Weka, J48, allows postpruning
of branches where the confidence of some of the leaves' error rates is questionable
(as discussed in section \ref{sec:data-reduction}).

Putting our full, non-reduced relation back into Weka, we can try the J48
classifier and see if it gives better results than straight ID3. The two parameters
that seem to be the most interesting are the \emph{confidence factor} and the
minimum number of objects per leaf. The confidence factor relates to the
confidence intervals discussed in section~\ref{sec:data-reduction} and
the minimum object per leaf helps prevent leaves that only classify one or two
recipes. These are likely candidates for overfitting and by replacing the whole
subtree with the leaf for the majority class thereunder, we might produce a model
better for the general recipe domain, as mentioned towards the end of
section~\ref{sec:idt-classification}.

Initially, the default values of 0.25 for the confidence factor and 2 for the minimum
number of objects per node are used to see if there are any immediate improvements
over ID3. In this case, the cross-validated classification accuracy comes out
as 59.9622\%. This is already somewhat better than straight ID3, but it should be possible to
tweak the results further. We can try another run with the confidence factor dropped
to 0.1. In this case, the accuracy increases to 61.2232.

The CART algorithm is another classifier that operates pruning, but not using
the loose statistical basis for C4.5's pruning. It uses a \emph{cost-complexity}
postpruning approach that works up from the leaves to the root, deciding if
the increased accuracy for each branch justifies the extra complexity from it
existing at all. This clearly favours a simpler tree, trading off local accuracy
perhaps for a simpler, more accurate model overall.

Weka provides a SimpleCart
implementation of this, which gives an accuracy of 64.691\%, which is the best
model achieved so far. It also produces far shallower trees than both ID3
and C4.5 (J48) for this particular problem. We will elect this one
(figure~\ref{cart-decision-tree}) as our best
classifier model so far for deeper evaluation.

\begin{sidewaysfigure}
  \centering
  \includegraphics[width=\linewidth]{cart.png}
  \label{fig:cart-decision-tree}
  \caption{Full decision tree as built from CART algorithm}
\end{sidewaysfigure}

\subsection{Confusion}

Taking the run from Weka's SimpleCart classifier (as it was the run with the 
best accuracy), we can view the confusion matrix in table~\ref{tab:confusion}
to see where the errors still lie and how they fit in with our understanding of domain.

\begin{sidewaystable}
  \centering
  \begin{tabular}{r r r r r r r r r r r r r r r r r r r | l}
    a&   b&   c&   d&   e&   f&   g&   h&   i&   j&   k&   l&   m&   n&   o&   p&   q&   r&   s& $\leftarrow$ classified as \\
    \hline
    0&   0&   7&   0&   0&   0&   0&   0&   5&   0&   4&   1&   1&   0&   0&   0&   0&   0&   0& a = african \\
    0&  30& 108&   4&   5&   0&   2&   0&   5&   0&  17&   0&   2&   1&   0&   0&   2&   2&   0& b = american \\
    0&   2&1749&   4&   7&   0&  15&   0&  18&   0& 121&   1&   1&   0&   0&   0&   5&   5&   0& c = british \\
    0&   0&  32&  31&   5&   0&   1&   0&   9&   0&   2&   0&   1&   1&   0&   0&   1&   3&   0& d = caribbean \\
    0&   0&  18&   1& 111&   0&   1&   0&   6&   0&   7&   5&   0&   1&   0&   0&   0&  12&   0& e = chinese \\
    0&   2&  47&   0&   0&   0&   2&   1&   0&   0&   6&   0&   1&   0&   0&   0&   4&   0&   0& f = east\_european \\
    0&   2& 399&   3&   0&   0&  63&   4&   3&   0&  52&   0&   0&   0&   0&   0&   1&   1&   0& g = french \\
    0&   0&  22&   0&   0&   0&   1&  10&   0&   0&  16&   0&   0&   0&   0&   0&   2&   0&   0& h = greek \\
    0&   1&  55&   1&   1&   0&   0&   0& 303&   0&   8&   0&   1&   1&   0&   0&   0&   8&   0& i = indian \\
    0&   0&  44&   0&   0&   0&   0&   0&   0&   0&   1&   0&   0&   0&   0&   0&   0&   0&   0& j = irish \\
    0&   1& 175&   3&   0&   0&   4&   0&   4&   0& 448&   1&   0&   0&   0&   0&   4&   1&   0& k = italian \\
    0&   0&  12&   2&  14&   0&   0&   0&   2&   0&   3&  26&   0&   0&   0&   0&   0&   6&   0& l = japanese \\
    0&   0&  16&   0&   0&   0&   0&   0&   9&   0&   6&   0&  26&   2&   0&   0&   3&   4&   0& m = mexican \\
    0&   0&  19&   0&   0&   0&   0&   0&  16&   0&   7&   0&   3&  24&   0&   0&   1&   2&   0& n = north\_african \\
    0&   0&   6&   0&   0&   0&   1&   0&   1&   0&   3&   0&   1&   0&   0&   0&   2&   0&   0& o = portuguese \\
    0&   0&  16&   2&   0&   0&   0&   0&   1&   0&   5&   0&   2&   0&   0&   0&   0&   1&   0& p = south\_american \\
    0&   0&  28&   0&   0&   0&   2&   3&   0&   0&  33&   0&   0&   3&   0&   0&  77&   0&   0& q = spanish \\
    0&   0&  15&   0&  17&   0&   1&   0&  24&   0&   6&   6&   2&   0&   0&   0&   0& 177&   0& r = thai\_and\_south-east\_asian \\
    0&   0&  15&   0&   0&   0&   0&   2&   5&   0&  11&   0&   2&   1&   0&   0&   2&   0&   3& s = turkish\_and\_middle\_eastern \\
  \end{tabular}
  \caption{Confusion matrix for cuisine classification with CART.}
  \label{tab:confusion}
\end{sidewaystable}

The confusion matrix shows many recipes are classified as British, when they are in
fact -- in descending order of confusion -- French, Italian, American or Indian. This
supports the knowledge that British cooking is the most likely to borrow
from other cultures. Chinese food has the biggest confusion with British (again,
another culture from which we borrow a lot of cuisine) and Thai/South-East Asian (a
reasonably close neighbour geographically). Spanish is confused mostly with British
and Italian. In short, the most difficult classifications are ones we would predict
with domain knowledge.

\subsection{Evaluating on new recipes}
\label{sec:evaluation}

Using the decision tree produced by the CART algorithm in figure~\ref{fig:cart-decision-tree},
we can do some manual evaluation on the uncategorised recipes. Ten recipes are chosen
at random and the cuisine classification from the decision tree is compared to best
judgement from knowledge of the food domain.

The results of the evaluation are shown in table~\ref{tab:evaluation}. One difficulty
in the manual classification was that the overlapping boundary between French and British -- as
mentioned briefly in section~\ref{sec:intro} -- caused some recipes to classified
as British, but some would argue that they are French in origin.

\begin{sidewaystable}
  \centering
  \begin{tabular}{| p{9cm} | p{3cm} | p{9cm} |}
    \hline
    \textbf{Recipe} & \textbf{Classification from model} & \textbf{Human classification} \\
    \hline
    Stuffed tomato with tuna, mozzarella and gremolata & Italian & Italian \\
    \hline
    N’duja-crusted cod with broccoli & Italian & Somewhat Italian. The chef and the 'Nduja sausage are Italian. \\
    \hline
    Rump steak, chips and béarnaise sauce & British & Mainly British, but the sauce is technically French in origin. \\
    \hline
    Long Island iced tea (simple version) & Chinese & American \\
    \hline
    Baked bass with creamed leeks & British & British, but could pass as French \\
    \hline
    Duck \'a l'orange with grilled broccoli & British & Canard \'a l'Orange is a classic French dish, but arguably Duck \'a l'orange is a British interpretation of it made popular in the 1960s. \cite{hestonsfeasts} \\
    \hline
    Thai green chicken curry with crisp carrot strips & Chinese & Thai/South-East Asian as it's a Thai curry by name, although it's thrown the model off by lacking cumin and fish sauce \\
    \hline
    Broccoli with chilli and garlic oil & Italian & Possibly Chinese due to wok style \\
    \hline
    Vanilla couscous scones served with rum and strawberry jam & British & British \\
    \hline
    Slow-cooked beef fillet with horseradish mash, wild mushroom tortellini and red wine sauce & British & British, despite things borrowed from other cultures originally \\
    \hline
  \end{tabular}
  \caption{Classifying random, uncategorised BBC recipes using CART-generated model}
  \label{tab:evaluation}
\end{sidewaystable}

Another clear observation is that the model utterly fails to classify drinks and cocktails.
This is due to the relatively low number of recipes that are cocktails compared
to food dishes, so the model achieves a better error rate overall focusing on the latter
only. This is a pervasive problem in that any grouping with a high number of instances
dominates the model more than fringe groups. The biggest blind spot that comes out
of this is that Portuguese, African, East European and South American recipes are simply
never classified with the final model. Their low incidence was predicted in
section~\ref{sec:exploring} to be problematic for classifying those cuisines and this
indeed seems to be the case.

As dicussed in section~\ref{pruning} with regard to pruning and specifically the CART
algorithm, a lower error rate on unseen examples is generally achieved by pruning
classifier branches that only classify a small number of instances in the training set.
The rationale is to avoid overfitting to noise, but this means that cuisines with
very few recipes are smoothed out along with that noise.

The accuracy rate is arguably 70\% for this admittedly small evaluation sample. This
is not far from the approximately 65\% accuracy predicted with cross-validation in
section~\ref{sec:pruning}. However, a problem with this evaluation is that
it suffers from the same British bias as the
recipes learning dataset. The classifier has learnt that a recipe is far more
likely to be British and the model produced is thus inclined to guess British
in cases where it can't otherwise narrow down typical foods for other cuisines. The
random sampling performed for this evaluation has also picked a large proportion
of British items, so a favourable evaluation is hardly surprising (a model that
always guessed British would have passed this evaluation with a 50\% accuracy).

This British bias problem is discussed further in section~\ref{sec:british-bias}.

\section{Discussion and Difficulties}
\subsection{Using ingredients only as attributes}

One of the first difficulties that surfaced was that some recipes simply could not
be told apart by ingredients alone. The example that came up was the fact that
cr\`eme br\^ul\'ee and amaretto vanilla custard had identical ingredients, despite
supposedly being French and British respectively. Do they differ due to method? Is
it solely in the names?

The models may have been more predictive if further attributes were retrieved.
Other attributes were identified, but omitted for simplicity, such as whether
the dish is a starter or dessert. If the model were to be specific to BBC recipe
classification, we could include the name of the chef and the programme
that showcased the recipe -- it might be that chefs or programmes tend toward
certain cuisines. A very sophisticated approach using natural language techniques
on the titles and the cooking method instructions could provide even more
insight.

The other simplification that was made was that all ingredients were equal. Strictly,
in the data model (see figure~\ref{model}) an \emph{ingredient} is a combination of a
\emph{food} and the quantity required for the recipe. Throughout this classification
problem, the term ingredient has been used to refer to an attribute indicating
whether or not a recipe has that ingredient or not -- i.e. a binary attribute. It may
be possible with a significant amount of normalisation and discretisation to
differentiate, say, 50g of sugar from 200g, which could be very different between
bread, cakes and American muffins, for example.

One small project that attempted to discretise ingredients was a small project
called Cover:Cheese. \cite{covercheese} The project uses a genetic algorithm
to adapt and learn appropriate recipes. Rather than numerical attributes over
mixed units, the application breaks ingredients into nominal values of \emph{small},
\emph{medium} and \emph{large} which are relative to normal expectations for
that particular ingredient.

\subsection{British cuisine and other biases}
\label{sec:british-bias}

A significant British bias for BBC recipes is shown in figure~\ref{cuisines-barchart}
and discussed in the evaluation in section~\ref{sec:evaluation}. Unsurprisingly
for the BBC, a large proportion of the recipes are British cuisine. The problem
is compounded as traditionally French, Italian or even Indian dishes have been
fully adopted into British culture as being British in their own right (and
usually altered from their traditional versions over time) due to the
amount of time that has passed since they were first imported. This is also
reflected in the English language with a large number of words of foreign origin
now being fully considered English in the modern age.

Machine learning and classification is a problem with classes of such vastly
different sizes. \cite{provost2000machine} Some have attempted to solve this
problem with ``downsampling'' the larger classes (i.e. remove instances
randomly until there are few as the smaller sets) or ``upsampling'' (i.e.
duplicate instances in the smaller sets so that they compete in size with
the larger classes). \cite{provost2000machine}

A downsampling approach is not ideal for the recipe problem as it would
dramatically reduce the number of training instances overall (e.g. if
all classes were downsampled to only 14 recipes each to match Portuguese). An
upsampling approach might have better results, but it will artificially
make the smaller cuisines seem less diverse than they really are and it
is beyond the scope of this report.

There are other more subtle biases in the data set within subsets from other
potential classifications that were discarded for this problem. For example,
main courses significantly outnumber desserts and cakes and food dishes
greatly outnumber drinks and cocktails (as noted in section~\ref{sec:evaluation}).
Underrepresented groups such as drinks and desserts do not classify well
on a decision tree trained on recipes where foods and main courses were in
the majority. This might be a sign of the need for ensemble methods as discussed
in section~\ref{sec:ensemble}.

It may be the case that it would have been better to retain categorical
attributes such as course, occasion, etc. in the relation used such that
a classifier could choose to branch on these before breaking down by
ingredient if it deemed that a useful split. However, this would still
be subject to the biases between those classes (e.g. a split between
food and drink is unlikely to be chosen if drinks are a very small
proportion of the overall set).

\subsection{Ensemble methods}
\label{sec:ensemble}

The recipe dataset has multiple groups that can be identified within it, e.g.
cuisines, occasions are shown in section~\ref{visualisation}. Classification
within these different groups might vary with, for example, French desserts
differing somewhat from French main courses in ingredients, despite both
being within the French cuisine class. This is a good candidiate for ensemble
methods where we combine different machine learning techniques.

A good way to improve the classification developed earlier could be to do
some unsupervised learning such as \emph{clustering} on the sets first. This
will break the recipes into unlabelled groups based on their similarity and
we can then run classification learning algorithms on the separate clusters.

This allows each classifier to become an expert in only one subdomain of
the recipe domain as a whole, where each subdomain is learnt through
unsupervised learning rather than splitting the set in known ways. This might
lead to distinctions a human expert may never have tried.

\subsection{Comparing decision tree classifiers}

\subsection{Interpreting output from Weka}

\subsection{Further Possibilties}

\bibliographystyle{cell}
\bibliography{bibtex}
\end{document}
